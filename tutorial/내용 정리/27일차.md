# 흔한 찐따의 파이썬 27일차 내용 정리

그동안 웹에 대해 공부했던 내용들을 한번에 정리하였다.

예전부터 프로젝트를 진행하면서 웹 기술을 활용한 프로젝트들이 대부분이였다.

그러면서 자연스럽게 웹에 대해 이해하는 것이 중요하다는 것을 깨달았다.

대부분의 데이터들은 모두 웹에 존재하기 때문에 웹에 대한 기술을 익혀두면 굉장히 유용하다.

## 웹 (Web)

- 인터넷에 연결된 사용자들이 서로의 정보를 공유할 수 있는 공간을 의미한다.
- 간단히 줄여서 WWW나 W3라고도 부르며, 간단히 **웹(Web)** 이라고 가장 많이 불린다.
- 인터넷과 같은 의미로 많이 사용되고 있지만, 정확히 말해 웹은 인터넷상의 인기 있는 하나의 서비스일 뿐이다.
- 하지만 현재에는 인터넷과 웹이라는 단어가 서로 혼용되어 사용될 만큼 인터넷의 가장 큰 부분을 차지하고 있다.

## HTTP 프로토콜

- **HTTP(HyperText Transfer Protocol)** 는 W3 상에서 정보를 주고받을 수 있는 프로토콜이다.
- 주로 HTML 문서를 주고받는 데에 쓰인다.

## URI와 URL

### URI

- **URI(Uniform Resource Identifier)** 는 인터넷에 있는 자원을 나타내는 유일한 주소이다.
- 쉽게 말해, URI는 인터넷 상의 자원을 식별하기 위한 문자열의 구성이다.

### URL

- **URL(Uniform Resource Locator)** 은 네트워크 상에서 자원이 어디 있는지를 알려주기 위한 규약이다.
- 쉽게 말해서, 웹 페이지를 찾기위한 주소를 말한다.

## 요청 방식 (Request Method)

HTTP 요청 방식은 많지만, 그 중에서 가장 많이 쓰이는 방식으로 2가지( **GET** , **POST** )가 존재한다.

- **GET** : 데이터 획득
  - 웹 서버에 데이터 전송을 요구
  - 요청한 데이터를 서버로부터 가져오기만 함
  - URL에 직접적으로 쿼리 문자열(parameter)을 이어 붙이는 방식을 취함
- **POST** : 데이터 전송
  - 클라이언트 -> 웹 서버로 데이터를 전송
  - 파일 전송이 가능
  - 데이터를 요청 메시지의 body에 담아 전송

## HTTP 상태 코드 (Status Code)

상태 코드는 1xx ~ 5xx 까지 존재한다.

- 1xx : 조건부 응답
- 2xx : 성공
- 3xx : 리다이렉션 완료
- 4xx : 요청 오류
- 5xx : 서버 오류

## 세션(Session)과 쿠키(Cookie)

### 세션 (Session)

- **세션(Session)** 은 일정 시간동안 같은 사용자(브라우저)로부터 들어오는 일련의 요구를 하나의 상태로 보고, 그 상태를 일정하게 유지시키는 기술이다.
- 여기서 일정 시간은 방문자가 웹 브라우저를 통해 웹 서버에 접속한 시점으로부터 웹 브라우저를 종료하여 연결을 끝내는 시점을 말한다.
- 즉, 방문자가 웹 서버에 접속해 있는 상태를 하나의 단위로 보고 그것을 세션이라고 한다.

### 쿠키 (Cookie)

- **쿠키(Cookie)** 는 HTTP의 일종으로 사용자가 어떠한 웹 사이트를 방문할 경우, 그 사이트가 사용하고 있는 서버에서 사용자의 컴퓨터에 저장하는 작은 기록 정보 파일이다.
- HTTP에서 클라이언트의 상태 정보를 클라이언트의 PC에 저장하였다가 필요시 정보를 참조하거나 재사용할 수 있다.

## CSS

**CSS**는 *Cascading Style Sheet*의 약어로써, HTML 등 마크업으로 작성된 문서가 실제 웹 사이트에 표현되는 방법을 정해주는 언어이다.

### CSS Selector

- Selector는 선택자라는 의미로써, 말 그대로 선택을 해주는 요소이다.
- CSS 객체와 CSS 객체에 대한 요소를 선택한다.
- CSS Selector 문법은 [여기](https://www.w3schools.com/cssref/css_selectors.asp)에서 확인할 수 있다.

# 파이썬(Python) HTTP 요청 및 응답

파이썬에서 HTTP 요청과 응답을 하는 방법은 여러 가지가 존재한다.

## urllib

- `urllib` 는 URL 작업을 위한 여러 모듈을 모은 패키지이다.
  - `urllib.request` – URL을 열고 읽기 위한 모듈
  - `urllib.error` – `urllib.request` 에 의해 발생하는 예외를 포함하는 모듈
  - `urllib.parse` – URL 구문 분석을 위한 모듈
  - `urllib.robotparser` – `robots.txt` 파일을 구문 분석하기 위한 모듈
- 자세한 내용은 [공식 문서](https://docs.python.org/ko/3/library/urllib.html)를 참고하면 된다.

### 예시

아래는 `urllib` 라이브러리를 사용하여 구글(Google) 로고 이미지를 요청하여 다운로드하는 예시이다.

```python
# re: 정규식을 위한 파이썬 표준 라이브러리
import re

# urllib: URL 처리를 위한 파이썬 표준 라이브러리
from urllib.request import urlopen
from urllib.parse import urljoin


# 구글 페이지로 접속하여 HTML 코드를 받아온다.
url = 'https://www.google.com'
google = urlopen(url)

# 바이트 스트림 데이터를 불러온 후, UTF-8 형식으로 변환해준다.
byte_stream = google.read()
html = byte_stream.decode('utf8')

# 정규식을 사용하여 구글 페이지 로고의 URL을 탐색한다.
pattern = re.compile(r'src="(.+?)"')
src = pattern.search(html)
url_img = src.group(1)

# 구글 페이지 로고의 URL과 구글 도메인을 합쳐준 후에 로고 이미지에 접속한다.
url_logo = urljoin(url, url_img)
logo = urlopen(url_logo)

# 이미지 데이터, 즉 바이트 스트림을 받아온다.
logo_byte_stream = logo.read()

# 받아온 이미지 데이터를 이미지 파일로 저장시킨다.
with open('logo.png', 'wb') as file:
    file.write(logo_byte_stream)
```

## requests

- `requests` 는 파이썬용 HTTP 클라이언트 인터페이스를 위한 고수준 라이브러리이다.
- [파이썬 공식 문서](https://docs.python.org/ko/3/library/urllib.request.html)에 따르면, `urllib` 라이브러리 대신 `requests` 라이브러리를 사용할 것을 권장하고 있다.
- `requests` 라이브러리를 사용하기 위해서는 명령 프롬프트에 `pip` 명령어를 통해 패키지를 설치한 다음 사용하면 된다.
  - 설치 명령어: `pip install requests`
- 자세한 내용은 [공식 문서](https://requests.readthedocs.io/en/latest/)를 참고하면 된다.

### 예시

아래는 `requests` 라이브러리를 사용하여 구글(Google) 로고 이미지를 요청하여 다운로드하는 예시이다.

```python
# re: 정규식을 위한 파이썬 표준 라이브러리
import re
# requests: 파이썬용 HTTP 클라이언트 인터페이스를 위한 고수준 라이브러리
# 파이썬 공식 문서에 따르면, 'urllib' 라이브러리 대신 'requests' 라이브러리를 사용할 것을 권장하고 있다.
# 참고: https://docs.python.org/ko/3/library/urllib.request.html
# requests 공식 문서: https://requests.readthedocs.io/en/latest/
# 사용하기 위해서는 'pip' 명령어를 통해 패키지를 설치한 다음 사용하면 된다.
# 설치 명령어: pip install requests
import requests

# urllib: URL 처리를 위한 파이썬 표준 라이브러리
from urllib.parse import urljoin


url = 'https://www.google.com'
# 'get' 함수를 통해 GET 방식으로 url을 요청한다.
# GET 방식이란, 'url?파라미터1=값1&파라미터2=값2&...' 형식으로 웹 서버에 HTML 문서를 요청하는 방식을 의미한다.
google = requests.get(url)
# 응답받은 Response 객체에서 'text' 프로퍼티에 접근하면 해당 URL의 HTML 문서를 가져올 수 있다.
html = google.text

# 정규식을 사용하여 구글 페이지 로고의 URL을 탐색한다.
pattern = re.compile(r'src="(.+?)"')
src = pattern.search(html)
url_img = src.group(1)

# 구글 도메인과 구글 페이지 로고의 URL을 합쳐준다.
# 해당 링크에 다시 요청을 보내면 바이너리 형식의 결과값을 얻을 수 있는데, 이미지를 웹에서 표현할 때에는 바이너리 형식으로 표현이 된다.
# 먼저, src 속성값의 링크는 URL이다.
# 단순 URL만으로는 어떤 도메인에 위치한 리소스인지 알 수 없으므로 이 도메인 주소와 합쳐준다.
# 이 작업을 URL 파싱(parsing)이라고 한다.
url_logo = urljoin(url, url_img)
logo = requests.get(url_logo)

# Byte Stream 데이터는 'content' 프로퍼티를 통해 접근할 수 있다.
byte_stream = logo.content

# 받아온 이미지 데이터를 'logo.png' 이미지 파일로 저장시킨다.
with open('logo.png', 'wb') as file:
    file.write(byte_stream)
```

### URL Encoding

- URL을 통해 서버로 요청을 보낼 때, 유니코드(unicode) 데이터를 바이트로 변환한 후 **퍼센트 인코딩**을 수행한다.
  - ex) 파이썬 -> %ED%8C%8C%EC%9D%B4%EC%8D%AC
- 구글에 파이썬이라는 키워드로 **GET** 방식을 통해 검색을 하게 된다면 다음과 같은 URL의 형태가 된다.

```
https://www.google.com/search?q=%ED%8C%8C%EC%9D%B4%EC%8D%AC
```

- 이것을 `requests` 라이브러리의 `get` 메서드를 사용하여 파라미터를 넘길 때, 다음과 같이 사용한다.

```python
import requests


url = 'http://www.google.co.kr'
params = {'q': '흔한 찐따'}
result = requests.get(url, params=params)
...
```

## html

- `html` 라이브러리는 하이퍼텍스트 마크업 언어(HTML) 지원을 위한 파이썬 표준 라이브러리이다.
- `html` 라이브러리는 HTML을 조작하는 유틸리티를 정의한다.
- `html` 패키지의 서브 모듈은 다음과 같다.
  - `html.parser` – 관대한 구문 분석 모드가 있는 HTML/XHTML 구문 분석기
  - `html.entities` – HTML 엔티티 정의
- 자세한 내용은 [공식 문서](https://docs.python.org/ko/3/library/html.html)를 참고하면 된다.

### 예시

아래는 `html.parser` 모듈로 HTML 구문 분석을 하는 간단한 예시이다.

```python
# html.parser: 관대한 구문 분석 모드가 있는 HTML/XHTML 구문 분석기
from html.parser import HTMLParser


# HTML로부터 Tag와 Data를 출력하는 'MyHTMLParser' 클래스를 정의한다.
class MyHTMLParser(HTMLParser):
    def handle_starttag(self, tag, attrs):
        print("시작 태그:", tag)

    def handle_endtag(self, tag):
        print("종료 태그:", tag)

    def handle_data(self, data):
        print("데이터:", data)


# 테스트를 위한 HTML 문서를 정의한다.
html = '''
<html>
<head>
    <title>Test</title>
</head>
<body>
    <h1>흔한 찐따의 HTML 구문 분석 예시</h1>
    <p>흔한 찐따의 HTML 구문 분석 튜토리얼</p>
    <p>안녕하세요, 흔한 찐따입니다.</p>
    <p>이것은 파이썬 HTML 구문 분석 테스트입니다.</p>
</body>
</html>
'''

# HTMLParser 객체를 생성한다.
parser = MyHTMLParser()

# HTML 문서의 구문을 분석한 후에 결과를 출력시킨다.
parser.feed(html)
```

아래는 `requests` 라이브러리를 사용하여 구글(Google) 로고 이미지를 요청한 후, `html.parser` 라이브러리를 통해 HTML 구문 분석을 하여 이미지 태그를 찾아낸 후 다운로드하는 예시이다.

```python
# requests: 파이썬용 HTTP 클라이언트 인터페이스를 위한 고수준 라이브러리
import requests

# urllib: URL 처리를 위한 파이썬 표준 라이브러리
from urllib.parse import urljoin
# html.parser: 간단한 HTML과 XHTML 구문 분석기를 위한 파이썬 표준 라이브러리
from html.parser import HTMLParser

# 구글 로고 이미지 URL 추출을 위한 'GoogleLogoHTMLParser' 클래스를 정의한다.
class GoogleLogoHTMLParser(HTMLParser):

    # 구글 로고의 img 태그의 속성들을 지정하기 위한 변수를 선언한다.
    attrs = None

    def handle_starttag(self, tag, attrs):
        # 태그가 img 태그일 경우에 attrs 변수에 태그 속성들을 지정한다.
        if tag == 'img':
            for attr, value in attrs:
                # 구글 로고에 해당하는 alt 속성이 'Google'이다.
                if attr == 'alt' and value == 'Google':
                    self.attrs = attrs
                    break


url = 'https://www.google.com'
google = requests.get(url)
html = google.text

parser = GoogleLogoHTMLParser()
parser.feed(html)

for attr, value in parser.attrs:
    if attr == 'src':
        url_logo = urljoin(url, value)
        logo = requests.get(url_logo)

        byte_stream = logo.content
        with open('logo.png', 'wb') as file:
            file.write(byte_stream)

        break
```

## html5lib

- `html5lib` 는 HTML 파싱(parsing)을 위한 순수 파이썬 라이브러리이다.
- 모든 주요 웹 브라우저에서 구현되는 WHATWG HTML 사양을 준수하도록 설계되었다.
- `html5lib` 라이브러리를 사용하기 위해서는 명령 프롬프트에 `pip` 명령어를 통해 설치하면 된다.
  - 설치 명령어: `pip install html5lib`
- 자세한 내용은 [공식 문서](https://html5lib.readthedocs.io/en/latest/)를 참고하면 된다.

### 예시

아래는 `requests` 라이브러리를 사용하여 구글(Google) 로고 이미지를 요청한 후, `html5lib` 라이브러리를 통해 DOM-Tree Parser 객체를 만든 후 HTML 구문 분석을 하여 이미지 태그를 찾아낸 후 다운로드하는 예시이다.

```python
# requests: 파이썬용 HTTP 클라이언트 인터페이스를 위한 고수준 라이브러리
import requests

# html5lib: HTML 파싱(parsing)을 위한 순수 파이썬 라이브러리
# 모든 주요 웹 브라우저에서 구현되는 WHATWG HTML 사양을 준수하도록 설계되었다.
# html5lib 라이브러리를 사용하기 위해서는 명령 프롬프트에 'pip' 명령어를 통해 설치하면 된다.
# 설치 명령어: pip install html5lib
import html5lib

# urllib: URL 처리를 위한 파이썬 표준 라이브러리
from urllib.parse import urljoin


url = 'https://www.google.com'
google = requests.get(url)
html = google.text

# html5lib 라이브러리의 parse 함수를 통해 HTML 태그와 데이터를 파싱한다.
# 이때, treebuilder의 인자값으로 'dom'을 넘겨주면 트리 형태의 구조로 가져온다.
document = html5lib.parse(html, treebuilder='dom')
# getElementsByTagName 메서드를 통해 해당하는 모든 태그들을 가져온다.
img_tags = document.getElementsByTagName('img')
for tag in img_tags:
    # getAttribute 메서드를 통해 태그의 속성을 가져온다.
    src = tag.getAttribute('src')

    # HTML 문서에 정의되어 있는 모든 img 태그의 이미지들을 저장시킨다.
    url_img = urljoin(url, src)
    google_logo = requests.get(url_img)
    byte_stream = google_logo.content

    filename = src.split('/')[-1]
    # 해당 파일명으로 이미지 파일을 저장시킨다.
    with open(filename, 'wb') as file:
        file.write(byte_stream)
```

## lxml

- `lxml` 라이브러리는 XML 및 HTML 문서의 빠른 처리를 위한 라이브러리이다.
- 속도가 굉장히 빠르며, XML 및 HTML 문서의 구문 분석 중에 잘못된 태그를 처리해주기도 한다.
- `lxml` 라이브러리를 사용하기 위해서는 명령 프롬프트에 `pip` 명령어를 통해 설치하면 된다.
  - 설치 명령어: `pip install lxml`
- 자세한 내용은 [공식 문서](https://lxml.de/)를 참고하면 된다.

### 예시

아래는 `requests` 라이브러리를 사용하여 구글(Google) 로고 이미지를 요청한 후, `lxml` 라이브러리를 통해 XPath를 통해 HTML 구문 분석을 하여 이미지 태그를 찾아낸 후 다운로드하는 예시이다.

```python
# requests: 파이썬용 HTTP 클라이언트 인터페이스를 위한 고수준 라이브러리
import requests

# urllib: URL 처리를 위한 파이썬 표준 라이브러리
from urllib.parse import urljoin

# lxml: XML 및 HTML 문서의 빠른 처리를 위한 라이브러리
# 속도가 굉장히 빠르며, XML 및 HTML 문서의 구문 분석 중에 잘못된 태그를 처리해주기도 한다.
# lxml 라이브러리를 사용하기 위해서는 명령 프롬프트에 'pip' 명령어를 통해 설치하면 된다.
# 설치 명령어: pip install lxml
from lxml import html


url = 'https://www.google.com'
google = requests.get(url)

document = html.fromstring(google.content)

img_tags = document.findall('.//img')
for img in img_tags:
    src = img.get('src')
    url_logo = urljoin(url, src)

    google_logo = requests.get(url_logo)
    byte_stream = google_logo.content

    filename = src.split('/')[-1]
    with open(filename, 'wb') as file:
        file.write(byte_stream)
```

## cssselect

- 참고로, `lxml` 라이브러리는 `cssselect` 라이브러리를 추가하면 CSS Selector를 사용할 수 있도록 설계되어 있다.
- `cssselect` 라이브러리를 설치하면 `cssselect` 메서드를 사용하여 CSS Selector를 사용할 수 있다.
  - 설치 명령어: `pip install cssselect`
- `cssselect` 라이브러리에 대한 자세한 설명은 [공식 문서](https://cssselect.readthedocs.io/en/latest/)를 참고하면 된다.
- `lxml` 라이브러리에서 `cssselect` 를 사용하는 방법은 [이 문서](https://lxml.de/cssselect.html)를 참고하면 된다.

### 예시

아래는 `lxml` 라이브러리에서의 간단한 `cssselect` 사용 예시이다.

```python

# requests 라이브러리를 통해 GET 방식으로 HTTP 요청을 한다.
import requests

# urljoin 함수를 통해 URL을 처리한다.
from urllib.parse import urljoin
# lxml의 html 모듈을 사용한다.
from lxml import html


url = 'https://www.google.com'
selector = 'img'

google = requests.get(url)
docs = html.fromstring(google.content)
# cssselect 메서드를 통해 CSS Selector를 사용하여 'img' 태그를 찾아낸다.
tags_img = docs.cssselect(selector)
img = tags_img[0]
# 'img' 태그의 'src' 속성을 가져온다.
src = img.get('src')

url_img = urljoin(url, src)
logo = requests.get(url_img)
byte_stream = logo.content

with open('logo.png', 'wb') as file:
    file.write(byte_stream)
```

## bs4 — BeautifulSoup

- `bs4` 라이브러리는 DOM-Tree Parser 객체 `BeautifulSoup` 를 지원하는 라이브러리이다.
- `BeautifulSoup` 은 Parse-Tree 탐색, 검색 및 수정을 위한 몇 가지 간단한 방법과 파이쏘닉(Pythonic) 관용구를 제공한다.
- HTML 문서를 분석하고 필요한 것을 추출하기 위한 툴킷이다.
- `BeautifulSoup` 은 들어오는 문서를 유니코드로, 나가는 문서를 UTF-8로 자동 변환한다.
- 문서가 인코딩을 지정하지 않고 `BeautifulSoup` 이 인코딩을 감지하지 못하는 경우가 아니라면 인코딩에 대해 생각할 필요가 없다.
- 그런 다음 원본 인코딩을 지정하기만 하면 된다.
- `BeautifulSoup` 은 `lxml` 및 `html5lib` 와 같은 인기 있는 파이썬 파서 위에 있어 다양한 구문 분석 전략을 시도하거나 유연성을 위해 속도를 교환할 수 있다.
- `BeautifulSoup` 은 사용자가 제공한 모든 것을 구문 분석하고 트리 탐색 작업을 수행한다.
- `bs4` 라이브러리를 사용하기 위해서는 명령 프롬프트에서 `pip` 명령어를 통해 설치하면 된다.
  - 설치 명령어: `pip install bs4`
- 자세한 설명은 [공식 문서](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)를 참고하면 된다.

### `BeautifulSoup` 에서 지원하는 XML Parser의 종류

- `html.parser` : 빠르지만 유연하지 않아 단순한 HTML 문서에 사용
- `lxml` : 매우 빠르고 유연함
- `xml` : XML 파일에만 사용
- `html5lib` : 복잡한 구조의 HTML에 대해서 사용

### 예시

아래는 `requests` 라이브러리를 사용하여 구글(Google) 로고 이미지를 요청한 후, `bs4` 라이브러리에서 제공하는 `BeautifulSoup` 을 통해 DOM-Tree Parser 객체를 만든 후 CSS Selector로 HTML 구문 분석을 하여 이미지 태그를 찾아낸 후 다운로드하는 예시이다.

```python
# requests: 파이썬용 HTTP 클라이언트 인터페이스를 위한 고수준 라이브러리
import requests


# urllib: URL 처리를 위한 파이썬 표준 라이브러리
from urllib.parse import urljoin

# bs4: HTML 및 XML 문서에서 데이터를 가져오기 위한 파이썬 라이브러리
# html.parser 라이브러리와 lxml 라이브러리의 파싱(parsing) 기능을 사용할 수 있다.
# bs4 라이브러리를 사용하기 위해서는 명령 프롬프트에서 'pip' 명령어를 통해 설치하면 된다.
# 설치 명령어: pip install bs4
from bs4 import BeautifulSoup


url = 'https://www.google.com'
google = requests.get(url)
html = google.text

parser = BeautifulSoup(html, 'lxml')
img_tags = parser.find_all('img')

for tag in img_tags:
    # dict 타입처럼 Key 인덱싱을 통해 태그의 속성값을 가져올 수 있다.
    src = tag['src']
    url_logo = urljoin(url, src)
    google_logo = requests.get(url_logo)
    byte_stream = google_logo.content

    filename = src.split('/')[-1]
    with open(filename) as file:
        file.write(byte_stream)
```
